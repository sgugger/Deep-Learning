{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#pytorch packages\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "#To download the dataset for torchvision\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "#For plots\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will try to recognize hand-written digits, specifically the ones of the MNIST dataset, that contains overall 70,000 28-by-28-pixels pictures of hand-written digits. This dataset is easily accessible in pytorch via dataset.MNSIT. You just have to specify you want to download it if it's not already in the directory, and pytorch will process it to create a DataSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change to the directory of your choice.\n",
    "PATH = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the training and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_set = datasets.MNIST(PATH, train=True, download=True)\n",
    "tst_set = datasets.MNIST(PATH, train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the data in the training set first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_set.train_data), len(tst_set.test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is represented by a tensor of size 28 by 28, each value represents the color of the corresponding pixel, from 0 (black) to 255 (white). Torch tensors are the equivalent of numpy ndarrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 12 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     3\n",
       "    0     0     0     0     0     0     0     0    30    36    94   154   170\n",
       "    0     0     0     0     0     0     0    49   238   253   253   253   253\n",
       "    0     0     0     0     0     0     0    18   219   253   253   253   253\n",
       "    0     0     0     0     0     0     0     0    80   156   107   253   253\n",
       "    0     0     0     0     0     0     0     0     0    14     1   154   253\n",
       "    0     0     0     0     0     0     0     0     0     0     0   139   253\n",
       "    0     0     0     0     0     0     0     0     0     0     0    11   190\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0    35\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0    39\n",
       "    0     0     0     0     0     0     0     0     0     0    24   114   221\n",
       "    0     0     0     0     0     0     0     0    23    66   213   253   253\n",
       "    0     0     0     0     0     0    18   171   219   253   253   253   253\n",
       "    0     0     0     0    55   172   226   253   253   253   253   244   133\n",
       "    0     0     0     0   136   253   253   253   212   135   132    16     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 13 to 25 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "   18    18    18   126   136   175    26   166   255   247   127     0     0\n",
       "  253   253   253   253   253   225   172   253   242   195    64     0     0\n",
       "  253   253   253   253   251    93    82    82    56    39     0     0     0\n",
       "  253   198   182   247   241     0     0     0     0     0     0     0     0\n",
       "  205    11     0    43   154     0     0     0     0     0     0     0     0\n",
       "   90     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "  190     2     0     0     0     0     0     0     0     0     0     0     0\n",
       "  253    70     0     0     0     0     0     0     0     0     0     0     0\n",
       "  241   225   160   108     1     0     0     0     0     0     0     0     0\n",
       "   81   240   253   253   119    25     0     0     0     0     0     0     0\n",
       "    0    45   186   253   253   150    27     0     0     0     0     0     0\n",
       "    0     0    16    93   252   253   187     0     0     0     0     0     0\n",
       "    0     0     0     0   249   253   249    64     0     0     0     0     0\n",
       "    0    46   130   183   253   253   207     2     0     0     0     0     0\n",
       "  148   229   253   253   253   250   182     0     0     0     0     0     0\n",
       "  253   253   253   253   201    78     0     0     0     0     0     0     0\n",
       "  253   253   198    81     2     0     0     0     0     0     0     0     0\n",
       "  195    80     9     0     0     0     0     0     0     0     0     0     0\n",
       "   11     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 26 to 27 \n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "[torch.ByteTensor of size 28x28]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_set.train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to convert a torch tensor to a numpy array via the .numpy() command.\n",
    "\n",
    "Conversely, you can create a torch Tensor from a numpy array x via torch.Tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_set.train_data[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's then easy to see the corresponding picture via plt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20e197250f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(trn_set.train_data[0].numpy(), cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the corresponding label..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_set.train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pytorch neural network will expect the data to come in the form of minibatches of tensors. To do that, we use a pytorch object called DataLoader. It will randomly separate the pictures (with the associated label) in minibatches. If you have multiple GPUs, it also prepares the work to be parallelized between them (just change num_workers from 0 to your custom value). We only shuffle the data randomly for the training.\n",
    "\n",
    "First we need to explicitely ask our dataset to transform the images in tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsfms = transforms.ToTensor()\n",
    "trn_set = datasets.MNIST(PATH, train=True, download=True, transform=tsfms)\n",
    "tst_set = datasets.MNIST(PATH, train=False, download=True, transform=tsfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_loader = torch.utils.data.DataLoader(trn_set, batch_size=64, shuffle=True, num_workers=0)\n",
    "tst_loader = torch.utils.data.DataLoader(tst_set, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at an example. A data loader can be converted into an iterator and we can then ask him for a minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_example = next(iter(trn_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a minibacth containts two torch tensors: the first one contains the data (here our pictures) and the second one the expected labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mb_example[0].size(), mb_example[1].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that pytorch has automatically added one dimension to our images (the 1 in second position). It would be 3 if we had had the three usual channels for the colors (RGB). Pytorch puts this channel in the second dimension and not the last because it simplifies some computation.\n",
    "\n",
    "Let's see the first tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 9 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.7922  1.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.3020  0.9843  0.9922\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.7882  0.9922  0.9922\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8392  0.9922  0.9922\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.2941  0.9608  0.9922  0.9373\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.4824  0.9922  0.9922  0.6706\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0706  0.9098  0.9922  0.8118  0.0824\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.4235  0.9922  0.9922  0.7294  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.4000  0.9882  0.9922  0.7451  0.0157\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.8941  0.9922  0.9922  0.2588\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.3725  0.9961  0.9961  1.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0196  0.8471  0.9922  0.9922\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.2549  0.7216  0.9922\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0431  0.2667\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 10 to 19 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.5608  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.6118  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.3137  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.1922\n",
       " 0.5882  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.3804\n",
       " 0.2314  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.2118  0.9412\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.6510  0.9922\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.7882  0.9922\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.7882  0.9922\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.1843  0.9059  0.9922\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.1961  0.6745  0.9922  0.9922\n",
       " 0.3137  0.0000  0.0000  0.0000  0.0000  0.0000  0.6196  0.9961  0.9961  0.9961\n",
       " 0.9608  0.6118  0.4235  0.4235  0.4235  0.9216  0.9882  0.9922  0.9922  0.9922\n",
       " 0.9922  0.9922  0.9922  0.9922  0.9961  0.9922  0.9922  0.9922  0.9922  0.9137\n",
       " 0.6784  0.8784  0.9922  0.8275  0.6824  0.1804  0.4667  0.9922  0.9922  0.7804\n",
       " 0.0000  0.1333  0.2078  0.0980  0.0000  0.0000  0.7843  0.9922  0.9922  0.7804\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.3137  0.9686  0.9922  0.9922  0.6706\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.5020  0.9922  0.9922  0.9529  0.2196\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.9451  0.9922  0.9922  0.4000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.9451  0.9922  0.9922  0.2078  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.4471  0.9412  0.8275  0.0980  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "Columns 20 to 27 \n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.8941  0.5216  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.9922  0.4471  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.9922  0.3647  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.9922  0.3647  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.9922  0.3647  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.9922  0.3647  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.9922  0.3647  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.9922  0.3647  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.9216  0.1922  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.6118  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.1961  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "[torch.FloatTensor of size 28x28]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mb_example[0][0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that pytorch transformed the values that went from 0 to 255 into floats that go from 0. to 1.\n",
    "\n",
    "We can have a look at the first pictures and draw them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(0,4):\n",
    "    sub_plot = fig.add_subplot(1,4,i+1)\n",
    "    sub_plot.axis('Off')\n",
    "    plt.imshow(mb_example[0][i,0].numpy(), cmap='Greys')\n",
    "    sub_plot.set_title(mb_example[1][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another usual transformation we do before feeing the pictures to our neural network is to normalize the input. This means subtracting the mean and dividing by the standard deviation. We can either search for the usual values on Google or compute them from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1306604762738429, 0.30810780717887876)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = torch.mean(trn_set.train_data.type(torch.FloatTensor))/255.\n",
    "std = torch.std(trn_set.train_data.type(torch.FloatTensor))/255.\n",
    "mean,std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide by 255 to get the means of our data when it's convereted into floats from 0. to 1.\n",
    "\n",
    "Then we go back to creating a transfrom and add the normalization. Note that we use the same mean and std for the test set. Afterward, we reload our datasets, adding this transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsfms = transforms.Compose([transforms.ToTensor(), transforms.Normalize((mean,), (std,))])\n",
    "trn_set = datasets.MNIST(PATH, train=True, download=True, transform=tsfms)\n",
    "tst_set = datasets.MNIST(PATH, train=False, download=True, transform=tsfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_loader = torch.utils.data.DataLoader(trn_set, batch_size=64, shuffle=True, num_workers=0)\n",
    "tst_loader = torch.utils.data.DataLoader(tst_set, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we want to plot our digits, we will have to denormalize the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_example = next(iter(trn_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB4CAYAAADi1gmcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADWhJREFUeJzt3X2wzdUex/H3ylGem1QjdUtN3VO3K2p6nlupREl16TmXaGqIFMWdMROSTDPVhEhGE7mISc29o9FoNGmKcIcalEmulOFi9HAVEuJ3/2Cts7a9zz6nc/bev99ev8/rn77zPdu2Zvudb2uvRxNFESIiUv6OibsBIiJSGCroIiKBUEEXEQmECrqISCBU0EVEAqGCLiISCBV0EZFApLKgG2NaGmP+ZYzZY4zZZIzpEXebQmGM+ZMxZpEx5idjzAZjTPe42xQKY8xAY8xKY8w+Y8z0uNsTmhCe3VQWdGASsB9oBfwNmGyM+XO8TSp/xpgKYB4wH2gJ9AVmGWMqY21YOLYCY4BpcTckNKE8uyZtO0WNMU2B/wFtoyhafyQ3E/hvFEXDYm1cmTPGtAWWA82jIw+WMWYh8O8oikbE2riAGGPGAH+IoqhP3G0JRSjPbhp76JXAQVvMj1gNqIdef6aaXNtSN0Tkdwri2U1jQW8G/HRU7iegeQxtCc06YAfwd2NMQ2NMZ6AD0CTeZonUKIhnN40FfTfQ4qhcC2BXDG0JShRFB4BuQFdgOzAEmAtsibNdIjUJ5dmtiLsBMVgPVBhj/hhF0X+O5NoDa2NsUzCiKFrD4Z4NAMaYpcA/4muRSO2E8OymroceRdEe4J/AaGNMU2PMX4C/AjPjbVkYjDHtjDGNjDFNjDFDgdbA9JibFQRjTIUxphHQAGhw5HNOY6esKEJ4dlNX0I8YADTm8JjZHKB/FEXqoRdGL2Abhz/bjkCnKIr2xdukYAwH9gLDgJ5H4uGxtigsZf/spm7ZoohIqNLaQxcRCY4KuohIIFTQRUQCoYIuIhIIFXQRkUCUeg2rltTULNeZErWhz7Zmdf1sQZ9vbejZLZ5afbbqoYuIBEIFXUQkECroIiKBUEEXEQmECrqISCBU0EVEAqGCLiISCBV0EZFAqKCLiAQiNbed+Oe+79y5E4BZs2bl/TOLFi1y8fXXX+/iiy++GIArrrjC5Y45Rv9vlOTbunUrAJWVlS7Xu3dvACZNmhRLm0rl119/dfG0adMAePTRR11u2LBhAIwcOdLljjvuuBK1rjBUhUREAqGCLiISiFJfQRfbITwzZsxwcZ8+fQrynldeeaWLX3jhhaycMXU6q0gHHBVPKg/n2rx5s4snTpwIwMaNG11u5szD96M3bty4vn9Vop/dCy+80MVffPFFta/zh1JffPFFF19++eXFaVjt6HAuEZE0UUEXEQlE8EMuH3zwAZA5m71u3bqi/X0ff/yxi6+66qq6vEWiv7bmsmLFChf37dsXgFWrVuV8rf3aOmXKFJdr3759EVuXIZVDLt26dXPxd999B1T9XgA0atSoUH9V4p7dxYsXu/jaa691catWrQDo3r27y9mhpz179rjc8ccf7+L33nsPgMsuu6woba2BhlxERNIkmB76oUOHXLxjxw4Xd+zYEYAvv/zyd7/n2LFjXbxt2zYX2wnQXPxJ0Y8++sjFFRW1XvKfuF6Oz/+cV65cCVR9xpDZu8mnRYsWLva/1bRr166+Tcwn+B76vn37AHjjjTdcbujQoS4eN24cULX2vMAS9+wOHjzYxXZCGGDUqFEAjBgxwuXst8onnnjC5fxns0mTJlk5f6K1yNRDFxFJExV0EZFABLP1/+eff3bxqaeemve1djvv888/73L+0NNzzz0HwJ133ulyTZs2dXHz5s0BePrpp13u4MGDACxbtszl/OGJcvbbb7+5eNCgQS6ePHlynd/T//fq1KmTi7/99lugIGuiU+mHH34A4J133nG5c88918X+BGka+J+D//voP3OWHT7xj/xYsGCBix944AEAli5dmvVnkkI9dBGRQATTQ3///ffz/txfmmWXJ91xxx05X3vNNddk5U444QQXDx8+HICFCxe63JIlS7L+zPLly/O+Z9LZbx2PP/64y9XUK7cTxnPmzHG56667zsXjx4/PeG+oWkoH8NVXXwHJ6/mUizPPPBOAAwcOuNzevXtdXMAlimXB363tH6D39ttvA5m7QnPp0qWLi9evXw8k+9ujeugiIoFQQRcRCUTZr0P//vvvAbj66qtdzn5th6qvmHaYBaofarF27doFQMOGDbPex2d3jgHcfPPNWT/3hw3smu1anJuemLW8dkKpuom0Nm3aAPDWW2+5nD0rvrqDye69914A5s6dm/Pnn332GVC0IZcg16F/+OGHLu7cuTMAn3zyictdeumlLq7jgXG1lZhn1/J/R7t27epiuwPU/3muHaD+cJVdp37JJZe4nF8jikzr0EVE0kQFXUQkEGW/ysVu5/WHWXx2+35Nwyw+u868vvwDqko8tFUQ8+bNy/tzu/LH/wpaE7t23x9yad26tYvbtm37e5qYWnaLP8Ds2bNdfPvttwOxHSCVODfddJOLO3To4GK7ff+GG25wuZNPPhnI/F3dv3+/i7dv3w7AhAkTXG7AgAEFbnH9qIcuIhKIsuyh+4dv+YcQWf460XJc/x0yf8259eOPP7r41VdfBZLX80kKe8nzmjVrXG7q1KkursshdGnh7/q0+yWeeuopl7MHy/k99FyTyHYNOyTvOVUPXUQkECroIiKBKKshF7ud2T9z/JtvvgEy14m/+eabLj7//POL1p4NGzbk/flZZ53l4iKv/y0Key60f8G2v2XfXj7sT0jbten+v8eWLVtc7F+6a/kTfHYduuRmD4a6++67Xe6hhx5ycWVlZcnbVC7soXwAQ4YMATInRXNZu3ati/v16wdknodu96xA4RZT1Id66CIigSirnaK2J5frcJxzzjnHxfYQnWKx/1f2jyW1S5p8/tGdt9xyS23fPnG77c444wwX+73tXM477zwAzj77bJf79NNPXZzrc7I3wUDVgWZFWr5Y9jtF7YTcY4895nL+RKh/B2YMEvfs1pfdiQ5wwQUXAJkT+7NmzXKx3QVdJNopKiKSJiroIiKBKKtJUX/9ean5kx/2sKpcwwf+MIy/S62c+Zdi33fffXlfu27duoz/1saJJ57oYu0UzWYPdoOqr/X+bVr+hdtSWCeddJKL7QF/Sf69Vg9dRCQQKugiIoEoqyEXf3350fwznwsl1zALZJ4/fTR/eKKioqw+3mr5X+93797t4sGDBwPwyy+/uFyDBg2AzHX3/iXT5XhIWVzswVC33Xaby9nP9bXXXsvKSXHZi+L9Zzhpz7N66CIigQijCwncf//9BXsv2zOvba8c4JFHHgFy31xU7myvG+DBBx/MilevXu1y7dq1AzJ7jXZXHsC4ceOK1s7Q2J65P/luL0Nv1qxZLG2SzGf766+/dvHGjRsBOO2001zO351aCuqhi4gEQgVdRCQQiR9yOXTokIv9M6ALLdcEaE3DLFOmTHFxjx49gFpdAh2c9u3bZ+X8Q7xef/31vH/eP1wq7ZYtW+ZiO9Tin7ntX4YudWcvfx40aJDL2aESe/wEZA6vbNu2Let9/PPUR40aBWQu0HjmmWeAzL0WF110UX2anlf6qo+ISKBU0EVEApH4IRefP6N8NP9r0o033pj3fexVU/5QwLPPPuviXFv6fQ8//DAAPXv2dLlcJ0CmmT+EtXPnzryvbdmyZbGbk2j+efD+8IodYvSPWzj22GNL17DA+J+zPULh3XffzXpdTVfQ1cS/oNv+e61YscLlNOQiIiI1SnwP3Z9k7N69O5A5cWS98sorLs41Seef420nKnJdWFwdfwLUrr9O4wRobfXu3dvFuXo8/vrce+65p3QNSxD7TXH69Okut2rVKhffddddAPTv37+k7QrV559/7uL58+dn/XzixIkA3HrrrS53+umnu9juAfAP5/Lfp0uXLtX+3aW6rF4VSUQkECroIiKBKKsr6OxX1FJdxuqf6e1fcVfkoZYgrvHq0KGDixcvXpz1c/+6rtmzZ5ekTSTsCrpNmzYBmReZ57ow2x6nUAYS/ezaS+YBBg4cCMDUqVNdbsKECQD06tXL5fy15/aZ9i9At5fUl4CuoBMRSZPET4r67ETa6NGjXW7kyJEFee+XXnrJxf369QMyj7/VBGjtbN26FYClS5e6XH2XgYVqxowZQNWuRcg8yKyMeuZloWHDhi62vXGfvXh7/PjxLucvv7XH5y5ZsqRYTaw3VSkRkUCooIuIBKKshlzsEMiTTz7pcnZN6IIFC1yupmEYOyHiv86/DFbDAnVnD+XyD+fS55nfKaec4uIxY8bE2JL0sMO3L7/8ssvZ/SuLFi1yOX/Xp71zwf/3Shr10EVEAqGCLiISiLJah54SiV7LW5PNmzcD0KZNm7yv0zr0IJX1s5twWocuIpImZTUpKsnXqlUrIHMNdTFvmhKRKuqhi4gEQgVdRCQQmhRNHk0sFY8mRYtLz27xaFJURCRNVNBFRAKhgi4iEggVdBGRQJR6UlRERIpEPXQRkUCooIuIBEIFXUQkECroIiKBUEEXEQmECrqISCBU0EVEAqGCLiISCBV0EZFAqKCLiARCBV1EJBAq6CIigVBBFxEJhAq6iEggVNBFRAKhgi4iEggVdBGRQKigi4gEQgVdRCQQKugiIoFQQRcRCYQKuohIIFTQRUQC8X9kz+EBR4h/bgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(0,4):\n",
    "    sub_plot = fig.add_subplot(1,4,i+1)\n",
    "    sub_plot.axis('Off')\n",
    "    plt.imshow(mb_example[0][i,0].numpy() * std + mean, cmap='Greys', interpolation=None)\n",
    "    sub_plot.set_title(mb_example[1][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always a good idea to create a model as a subclass of nn.Module. That way, we can use all the features this class provides.\n",
    "\n",
    "We override the init function (but still call the init function of nn.Module) to define our custom layers (here two linear layers) and we have to define the forward function, which explains how to compute the output.\n",
    "\n",
    "The first line of the forward function is to flatten our input, since we saw it has four dimensions: minibatch by channel by height by width. We only keep the minibatch size as our first dimension (x.size(0)) and the -1 is to tell pytorch to determine the right number for the second dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(n_in, n_hidden)\n",
    "        self.linear2 = nn.Linear(n_hidden, n_out)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        return F.log_softmax(self.linear2(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can instanciate the class with our input size (28 * 28), an hidden size of 100 layers and 10 outputs (as many as digits).\n",
    "\n",
    "The optimizer will automatically do the Stochastic Gradient Descent for us (or any of its variant if we want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SimpleNeuralNet(28*28,100,10)\n",
    "optimizer = optim.SGD(net.parameters(),lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to write our training loop. To compute the gradient automatically, pytorch requires us to put the torch tensors with our inputs and labels into Variable objects, that way it'll remember the transformation these go through until we arrive at our loss function. We then call loss.backward() to compute all the gradients (which will then be in the grad field of any variable).\n",
    "\n",
    "The optimizer takes care of the step of our gradient descent in the optimizer.step() function. Since the gradients are accumulated, we have to tell pytorch when to reinitialize them (which the purpose of the optimizer.zero_grad() command at the beginning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(nb_epoch):\n",
    "    for epoch in range(nb_epoch):\n",
    "        running_loss = 0.\n",
    "        corrects = 0\n",
    "        print(f'Epoch {epoch+1}:')\n",
    "        for data in trn_loader:\n",
    "            #separate the inputs from the labels\n",
    "            inputs,labels = data\n",
    "            #wrap those into variables to keep track of how they are created and be able to compute their gradient.\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            #Put the gradients back to zero\n",
    "            optimizer.zero_grad()\n",
    "            #Compute the outputs given by our model at this stage.\n",
    "            outputs = net(inputs)\n",
    "            _,preds = torch.max(outputs.data,1)\n",
    "            #Compute the loss\n",
    "            loss = F.nll_loss(outputs, labels)\n",
    "            running_loss += loss.data[0] * inputs.size(0)\n",
    "            corrects += torch.sum(labels.data == preds)\n",
    "            #Backpropagate the computation of the gradients\n",
    "            loss.backward()\n",
    "            #Do the step of the SGD\n",
    "            optimizer.step()\n",
    "        print(f'Loss: {running_loss/len(trn_set)}  Accuracy: {100.*corrects/len(trn_set)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Loss: 0.5927764947732289  Accuracy: 84.855\n",
      "Epoch 2:\n",
      "Loss: 0.3036189037680626  Accuracy: 91.29666666666667\n",
      "Epoch 3:\n",
      "Loss: 0.25601380693117776  Accuracy: 92.66\n",
      "Epoch 4:\n",
      "Loss: 0.22408075144290923  Accuracy: 93.60333333333334\n",
      "Epoch 5:\n",
      "Loss: 0.19974409602483115  Accuracy: 94.35666666666667\n",
      "Epoch 6:\n",
      "Loss: 0.18035491447448732  Accuracy: 94.95833333333333\n",
      "Epoch 7:\n",
      "Loss: 0.16477193065285684  Accuracy: 95.36833333333334\n",
      "Epoch 8:\n",
      "Loss: 0.1516276248137156  Accuracy: 95.76833333333333\n",
      "Epoch 9:\n",
      "Loss: 0.14064246021111806  Accuracy: 96.075\n",
      "Epoch 10:\n",
      "Loss: 0.13113217855294546  Accuracy: 96.30166666666666\n"
     ]
    }
   ],
   "source": [
    "train(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "96.3% accuracy is good, but that's on the training set and we may be overfitting. Let's try on the test set now to see if we're doing well or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    running_loss = 0.\n",
    "    corrects = 0\n",
    "    for data in tst_loader:\n",
    "        #separate the inputs from the labels\n",
    "        inputs,labels = data\n",
    "        #wrap those into variables to keep track of how they are created and be able to compute their gradient.\n",
    "        #Even if we don't require the gradient here, a nn.Module expects a variable.\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        #Compute the outputs given by our model at this stage.\n",
    "        outputs = net(inputs)\n",
    "        _,preds = torch.max(outputs.data,1)\n",
    "        #Compute the loss\n",
    "        loss = F.nll_loss(outputs, labels)\n",
    "        running_loss += loss.data[0] * inputs.size(0)\n",
    "        corrects += torch.sum(labels.data == preds)\n",
    "    print(f'Loss: {running_loss/len(tst_set)}  Accuracy: {100.*corrects/len(tst_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.12863390633314847  Accuracy: 96.25\n"
     ]
    }
   ],
   "source": [
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we weren't overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate finder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The details of how this code has been built are all explained in this [blog article](https://sgugger.github.io/a-neural-net-in-pytorch.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lr(init_value = 1e-8, final_value=10., beta = 0.98):\n",
    "    num = len(trn_loader)-1\n",
    "    mult = (final_value / init_value) ** (1/num)\n",
    "    lr = init_value\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    avg_loss = 0.\n",
    "    best_loss = 0.\n",
    "    batch_num = 0\n",
    "    losses = []\n",
    "    log_lrs = []\n",
    "    for data in trn_loader:\n",
    "        batch_num += 1\n",
    "        #As before, get the loss for this mini-batch of inputs/outputs\n",
    "        inputs,labels = data\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        #Compute the smoothed loss\n",
    "        avg_loss = beta * avg_loss + (1-beta) *loss.data[0]\n",
    "        smoothed_loss = avg_loss / (1 - beta**batch_num)\n",
    "        #Stop if the loss is exploding\n",
    "        if batch_num > 1 and smoothed_loss > 4 * best_loss:\n",
    "            return log_lrs, losses\n",
    "        #Record the best loss\n",
    "        if smoothed_loss < best_loss or batch_num==1:\n",
    "            best_loss = smoothed_loss\n",
    "        #Store the values\n",
    "        losses.append(smoothed_loss)\n",
    "        log_lrs.append(math.log10(lr))\n",
    "        #Do the SGD step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #Update the lr for the next step\n",
    "        lr *= mult\n",
    "        optimizer.param_groups[0]['lr'] = lr\n",
    "    return log_lrs, losses\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our neural net as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SimpleNeuralNet(28*28,100,10)\n",
    "optimizer = optim.SGD(net.parameters(),lr=1e-1)\n",
    "criterion = F.nll_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then plot the losses versus the logs of the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs,losses = find_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20e1b4553c8>]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(logs[10:-5],losses[10:-5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests the best learning rate is $10^{-1}$ so we can use test this one after defining a new network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Loss: 0.2622879001150529  Accuracy: 92.20833333333333\n"
     ]
    }
   ],
   "source": [
    "net = SimpleNeuralNet(28*28,100,10)\n",
    "optimizer = optim.SGD(net.parameters(),lr=1e-1)\n",
    "train(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are already at 92.21% accuracy when the learning rate used before gave us 84.86% in one epoch!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
